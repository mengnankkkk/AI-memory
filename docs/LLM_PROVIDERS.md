# LLMæœåŠ¡æä¾›å•†åˆ‡æ¢æŒ‡å—

## ğŸ“‹ æ”¯æŒçš„æä¾›å•†

### 1. MockæœåŠ¡ (é»˜è®¤)
ç”¨äºæµ‹è¯•å’Œæ¼”ç¤ºï¼Œè¿”å›åŸºäºæ€§æ ¼çš„æ¨¡æ‹Ÿå›å¤

```bash
LLM_PROVIDER=mock
```

### 2. æ–°Gradio API â­ (æ¨è)
åŸºäº https://haiyangpengai-careyou.ms.show/

```bash
LLM_PROVIDER=new_gradio
NEW_GRADIO_API_URL=https://haiyangpengai-careyou.ms.show/
```

**ç‰¹ç‚¹**:
- âœ… çœŸå®AIå›å¤
- âœ… æ”¯æŒTTSè¯­éŸ³åˆæˆ
- âœ… å¯è°ƒèŠ‚å‚æ•° (temperature, top_p, max_tokens)

### 3. DeepSeek Gradio
åŸºäºHugging Face Space (éœ€è¦ç½‘ç»œè®¿é—®)

```bash
LLM_PROVIDER=deepseek_gradio
DEEPSEEK_API_URL=Mengnankk/deepseek-ai-DeepSeek-V3.1-test
```

## ğŸ”§ å¦‚ä½•åˆ‡æ¢

### æ–¹æ³•1: ä¿®æ”¹ `.env` æ–‡ä»¶

```bash
# ç¼–è¾‘ backend/.env
LLM_PROVIDER=new_gradio  # æ”¹ä¸ºä½ æƒ³è¦çš„æä¾›å•†
```

### æ–¹æ³•2: ç¯å¢ƒå˜é‡

```bash
# Windows PowerShell
$env:LLM_PROVIDER="new_gradio"
python -m uvicorn app.main:app --reload

# Linux/Mac
export LLM_PROVIDER=new_gradio
python -m uvicorn app.main:app --reload
```

## ğŸ“Š æä¾›å•†å¯¹æ¯”

| æä¾›å•† | çœŸå®AI | é€Ÿåº¦ | ç½‘ç»œè¦æ±‚ | æˆæœ¬ |
|--------|--------|------|----------|------|
| Mock | âŒ | âš¡âš¡âš¡ | âŒ | å…è´¹ |
| New Gradio | âœ… | âš¡âš¡ | âœ… | å…è´¹ |
| DeepSeek Gradio | âœ… | âš¡ | âœ… (éœ€ç¿»å¢™) | å…è´¹ |

## ğŸ› ï¸ è‡ªå®šä¹‰æä¾›å•†

### Step 1: åˆ›å»ºæœåŠ¡ç±»

åœ¨ `backend/app/services/llm/` ä¸‹åˆ›å»ºæ–°æ–‡ä»¶ `your_service.py`:

```python
from app.services.llm.base import BaseLLMService
from typing import List, Dict

class YourLLMService(BaseLLMService):
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> str:
        # å®ç°ä½ çš„APIè°ƒç”¨é€»è¾‘
        pass

    def get_provider_name(self) -> str:
        return "Your Service Name"
```

### Step 2: æ³¨å†Œåˆ°å·¥å‚

ç¼–è¾‘ `backend/app/services/llm/factory.py`:

```python
from app.services.llm.your_service import YourLLMService

# åœ¨ create_service æ–¹æ³•ä¸­æ·»åŠ 
elif provider == "your_service":
    return YourLLMService(api_url=settings.YOUR_API_URL)
```

### Step 3: æ›´æ–°é…ç½®

åœ¨ `.env` ä¸­æ·»åŠ :

```bash
LLM_PROVIDER=your_service
YOUR_API_URL=https://your-api-url.com
```

## ğŸ” è°ƒè¯•

### æŸ¥çœ‹å½“å‰æä¾›å•†

å¯åŠ¨åç«¯æ—¶ä¼šæ‰“å°:

```
âœ“ New Gradio API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ
```

### æµ‹è¯•APIè°ƒç”¨

```bash
cd backend
python -c "
from app.services.llm.factory import llm_service
import asyncio

async def test():
    messages = [
        {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªæ¸©æŸ”çš„åŠ©æ‰‹'},
        {'role': 'user', 'content': 'ä½ å¥½'}
    ]
    response = await llm_service.chat_completion(messages)
    print(response)

asyncio.run(test())
"
```

## âš ï¸ å¸¸è§é—®é¢˜

### Q: åˆ‡æ¢æä¾›å•†åæ²¡æœ‰ç”Ÿæ•ˆ?

A: éœ€è¦é‡å¯åç«¯æœåŠ¡:
```bash
# åœæ­¢å½“å‰æœåŠ¡ (Ctrl+C)
# é‡æ–°å¯åŠ¨
python -m uvicorn app.main:app --reload
```

### Q: New Gradio APIè°ƒç”¨å¤±è´¥?

A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹:
1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸
2. API URLæ˜¯å¦æ­£ç¡®
3. æŸ¥çœ‹åç«¯æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯

### Q: å¦‚ä½•å›é€€åˆ°Mockæ¨¡å¼?

A: ä¿®æ”¹ `.env`:
```bash
LLM_PROVIDER=mock
```
