"""
å“åº”åè°ƒå™¨ (Response Coordinator)
åŒé˜¶æ®µ"å¿ƒæµ"äº¤äº’åè®®çš„æ ¸å¿ƒè°ƒåº¦å™¨

èŒè´£ï¼š
1. åè°ƒæ•´ä¸ªåŒé˜¶æ®µæµç¨‹çš„æ‰§è¡Œ
2. æ•´åˆæƒ…æ„Ÿåˆ†æã€çŠ¶æ€æ›´æ–°ã€è¡¨ç°ç”Ÿæˆã€æç¤ºè¯æ„å»º
3. ç®¡ç†LLMè°ƒç”¨å’Œå“åº”ç”Ÿæˆ
4. æä¾›ç»Ÿä¸€çš„APIæ¥å£
"""
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import logging
import json

from app.services.affinity_engine import affinity_engine, EmotionAnalysis, ProcessResult
from app.services.emotion_expression_generator import emotion_expression_generator, EmotionExpression
from app.services.dynamic_prompt_builder import dynamic_prompt_builder
from app.services.llm.factory import llm_service
from app.services.memory_integration import memory_system

logger = logging.getLogger("response_coordinator")


@dataclass
class CoordinatedResponse:
    """åè°ƒåçš„å®Œæ•´å“åº”"""
    # æœ€ç»ˆAIå›å¤
    ai_response: str

    # é˜¶æ®µ1: æƒ…æ„Ÿåˆ†æå’ŒçŠ¶æ€æ›´æ–°
    emotion_analysis: EmotionAnalysis
    process_result: ProcessResult

    # é˜¶æ®µ2: æƒ…æ„Ÿè¡¨ç°å’Œæç¤ºè¯
    emotion_expression: EmotionExpression
    system_prompt: str

    # å…ƒæ•°æ®
    debug_info: Dict  # è°ƒè¯•ä¿¡æ¯


class ResponseCoordinator:
    """
    å“åº”åè°ƒå™¨

    æ ¸å¿ƒæµç¨‹ï¼š
    1. ã€é˜¶æ®µ1-æƒ…æ„Ÿåˆ†æã€‘ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ¶ˆæ¯æƒ…æ„Ÿå’Œæ„å›¾
    2. ã€é˜¶æ®µ1-çŠ¶æ€æ›´æ–°ã€‘è®¡ç®—å¹¶æ›´æ–°å¥½æ„Ÿåº¦ã€ä¿¡ä»»åº¦ã€ç´§å¼ åº¦
    3. ã€é˜¶æ®µ1-è®°å¿†æŸ¥è¯¢ã€‘ä»è®°å¿†ç³»ç»Ÿæ£€ç´¢ç›¸å…³è®°å¿†
    4. ã€é˜¶æ®µ2-è¡¨ç°ç”Ÿæˆã€‘ç”Ÿæˆè¯¦ç»†çš„æƒ…æ„Ÿè¡¨ç°JSON
    5. ã€é˜¶æ®µ2-æç¤ºè¯æ„å»ºã€‘æ„å»ºåŠ¨æ€ä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯
    6. ã€é˜¶æ®µ2-å“åº”ç”Ÿæˆã€‘ä½¿ç”¨LLMç”Ÿæˆæœ€ç»ˆå›å¤
    7. ã€åå¤„ç†ã€‘è®°å¿†å­˜å‚¨ã€çŠ¶æ€æŒä¹…åŒ–
    """

    def __init__(self):
        self.logger = logging.getLogger("response_coordinator")

    async def coordinate_response(
        self,
        # ç”¨æˆ·è¾“å…¥
        user_message: str,
        user_id: str,

        # ä¼™ä¼´ä¿¡æ¯
        companion_id: int,
        companion_name: str,

        # å½“å‰çŠ¶æ€
        current_affinity_score: int,
        current_trust_score: int,
        current_tension_score: int,
        current_level: str,
        current_mood: str = "neutral",

        # å¯é€‰å‚æ•°
        conversation_history: Optional[List[Dict]] = None,
        enable_memory: bool = True,
        special_instructions: Optional[str] = None,
        debug_mode: bool = False

    ) -> CoordinatedResponse:
        """
        åè°ƒå®Œæ•´çš„å“åº”ç”Ÿæˆæµç¨‹

        Returns:
            CoordinatedResponse: åŒ…å«AIå›å¤å’Œæ‰€æœ‰ä¸­é—´ç»“æœçš„å®Œæ•´å“åº”
        """
        self.logger.info(
            f"\n{'='*80}\n"
            f"ğŸš€ å¼€å§‹åè°ƒå“åº”ç”Ÿæˆ\n"
            f"{'='*80}\n"
            f"ç”¨æˆ·ID: {user_id} | ä¼™ä¼´: {companion_name} | ç­‰çº§: {current_level}\n"
            f"æ¶ˆæ¯: {user_message[:50]}...\n"
            f"{'='*80}"
        )

        debug_info = {
            "stages": {},
            "timings": {},
            "errors": []
        }

        try:
            # ==========================================
            # é˜¶æ®µ1: æƒ…æ„Ÿåˆ†æå’ŒçŠ¶æ€æ›´æ–°
            # ==========================================
            self.logger.info("\nğŸ“Š é˜¶æ®µ1: æƒ…æ„Ÿåˆ†æå’ŒçŠ¶æ€æ›´æ–°")

            # 1.1 æŸ¥è¯¢è®°å¿†ï¼ˆç”¨äºæƒ…æ„Ÿåˆ†æçš„ä¸Šä¸‹æ–‡ï¼‰
            memories = None
            user_facts = None
            if enable_memory:
                memories, user_facts = await self._query_memory(
                    user_id, companion_id, user_message
                )

            # 1.2 ä½¿ç”¨AffinityEngineè¿›è¡Œæƒ…æ„Ÿåˆ†æå’ŒçŠ¶æ€è®¡ç®—
            process_result = await affinity_engine.process_user_message(
                user_message=user_message,
                current_affinity_score=current_affinity_score,
                current_trust_score=current_trust_score,
                current_tension_score=current_tension_score,
                current_level=current_level,
                current_mood=current_mood,
                companion_name=companion_name,
                user_id=user_id,
                companion_id=companion_id,
                recent_memories=memories,
                user_facts=user_facts
            )

            self.logger.info(
                f"âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ: {process_result.emotion_analysis.primary_emotion} "
                f"(å¼ºåº¦:{process_result.emotion_analysis.emotion_intensity:.2f})"
            )
            self.logger.info(
                f"âœ… çŠ¶æ€æ›´æ–°: å¥½æ„Ÿåº¦ {current_affinity_score} â†’ {process_result.new_affinity_score} "
                f"({process_result.affinity_change:+d})"
            )

            debug_info["stages"]["stage1_analysis"] = {
                "emotion": process_result.emotion_analysis.primary_emotion,
                "intensity": process_result.emotion_analysis.emotion_intensity,
                "affinity_change": process_result.affinity_change,
                "new_affinity": process_result.new_affinity_score,
                "level_changed": process_result.level_changed
            }

            # ==========================================
            # é˜¶æ®µ2: æƒ…æ„Ÿè¡¨ç°ç”Ÿæˆå’Œæç¤ºè¯æ„å»º
            # ==========================================
            self.logger.info("\nğŸ­ é˜¶æ®µ2: æƒ…æ„Ÿè¡¨ç°å’Œæç¤ºè¯ç”Ÿæˆ")

            # 2.1 ç”Ÿæˆæƒ…æ„Ÿè¡¨ç°JSON
            emotion_expression = emotion_expression_generator.generate(
                emotion_analysis=process_result.emotion_analysis,
                current_level=process_result.new_level,
                affinity_score=process_result.new_affinity_score,
                trust_score=process_result.new_trust_score,
                tension_score=process_result.new_tension_score,
                mood=current_mood
            )

            self.logger.info(
                f"âœ… æƒ…æ„Ÿè¡¨ç°ç”Ÿæˆ: {emotion_expression.emotion_type} "
                f"({emotion_expression.intensity_level}å¼ºåº¦)"
            )

            debug_info["stages"]["stage2_expression"] = {
                "emotion_type": emotion_expression.emotion_type,
                "category": emotion_expression.emotion_category,
                "intensity_level": emotion_expression.intensity_level,
                "formality": emotion_expression.verbal_style['formality']
            }

            # 2.2 æ„å»ºL1å·¥ä½œè®°å¿†ï¼ˆå½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
            l1_working_memory = self._build_working_memory(conversation_history)

            # 2.3 æ„å»ºåŠ¨æ€ç³»ç»Ÿæç¤ºè¯
            system_prompt = dynamic_prompt_builder.build(
                companion_name=companion_name,
                emotion_expression=emotion_expression,
                emotion_analysis=process_result.emotion_analysis,
                current_level=process_result.new_level,
                affinity_score=process_result.new_affinity_score,
                trust_score=process_result.new_trust_score,
                tension_score=process_result.new_tension_score,
                mood=current_mood,
                l1_working_memory=l1_working_memory,
                l2_episodic_memories=memories,
                l3_semantic_facts=user_facts,
                special_instructions=special_instructions
            )

            self.logger.info(f"âœ… ç³»ç»Ÿæç¤ºè¯æ„å»ºå®Œæˆ (é•¿åº¦: {len(system_prompt)} å­—ç¬¦)")

            debug_info["stages"]["stage2_prompt"] = {
                "prompt_length": len(system_prompt),
                "has_memories": bool(memories),
                "has_facts": bool(user_facts)
            }

            # ==========================================
            # é˜¶æ®µ3: LLMç”Ÿæˆæœ€ç»ˆå›å¤
            # ==========================================
            self.logger.info("\nğŸ’¬ é˜¶æ®µ3: LLMç”Ÿæˆæœ€ç»ˆå›å¤")

            # 3.1 æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]

            # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œæ·»åŠ æœ€è¿‘å‡ è½®ï¼ˆæœ€å¤š3è½®ï¼‰
            if conversation_history:
                recent_history = conversation_history[-6:]  # æœ€è¿‘3è½®ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
                # åœ¨systemå’Œuserä¹‹é—´æ’å…¥å†å²
                messages = [messages[0]] + recent_history + [messages[1]]

            # 3.2 è°ƒç”¨LLM
            ai_response = await llm_service.chat_completion(messages)

            self.logger.info(f"âœ… AIå›å¤ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(ai_response)} å­—ç¬¦)")

            debug_info["stages"]["stage3_generation"] = {
                "response_length": len(ai_response),
                "message_count": len(messages)
            }

            # ==========================================
            # é˜¶æ®µ4: åå¤„ç†ï¼ˆè®°å¿†å­˜å‚¨ã€çŠ¶æ€æŒä¹…åŒ–ï¼‰
            # ==========================================
            if enable_memory and process_result.emotion_analysis.is_memorable:
                self.logger.info("\nğŸ’¾ é˜¶æ®µ4: å­˜å‚¨é‡è¦è®°å¿†")
                await self._store_memory(
                    user_id, companion_id,
                    user_message, ai_response,
                    process_result.emotion_analysis
                )

            # æ„å»ºæœ€ç»ˆå“åº”
            coordinated_response = CoordinatedResponse(
                ai_response=ai_response,
                emotion_analysis=process_result.emotion_analysis,
                process_result=process_result,
                emotion_expression=emotion_expression,
                system_prompt=system_prompt if debug_mode else "[éšè—]",
                debug_info=debug_info if debug_mode else {}
            )

            self.logger.info(
                f"\n{'='*80}\n"
                f"âœ… å“åº”åè°ƒå®Œæˆ\n"
                f"{'='*80}"
            )

            return coordinated_response

        except Exception as e:
            self.logger.error(f"âŒ å“åº”åè°ƒå¤±è´¥: {e}", exc_info=True)
            debug_info["errors"].append(str(e))

            # è¿”å›é™çº§å“åº”
            return await self._fallback_response(
                user_message, companion_name,
                debug_info, str(e)
            )

    async def _query_memory(
        self,
        user_id: str,
        companion_id: int,
        user_message: str
    ) -> Tuple[Optional[List[str]], Optional[Dict]]:
        """
        ä»è®°å¿†ç³»ç»ŸæŸ¥è¯¢ç›¸å…³ä¿¡æ¯

        Returns:
            (memories, facts): (L2æƒ…æ™¯è®°å¿†åˆ—è¡¨, L3ç”¨æˆ·äº‹å®å­—å…¸)
        """
        try:
            # æŸ¥è¯¢ç›¸å…³è®°å¿†ï¼ˆL2ï¼‰
            memories = await memory_system.get_recent_memories(
                user_id=user_id,
                companion_id=companion_id,
                query=user_message,
                limit=5
            )

            # è·å–ç”¨æˆ·äº‹å®ï¼ˆL3ï¼‰
            user_facts = await memory_system.get_user_facts(
                user_id=user_id,
                companion_id=companion_id
            )

            self.logger.info(
                f"ğŸ“š è®°å¿†æŸ¥è¯¢: {len(memories) if memories else 0} æ¡è®°å¿†, "
                f"{len(user_facts) if user_facts else 0} ä¸ªäº‹å®"
            )

            return memories, user_facts

        except Exception as e:
            self.logger.warning(f"è®°å¿†æŸ¥è¯¢å¤±è´¥: {e}")
            return None, None

    async def _store_memory(
        self,
        user_id: str,
        companion_id: int,
        user_message: str,
        ai_response: str,
        emotion_analysis: EmotionAnalysis
    ):
        """å­˜å‚¨é‡è¦è®°å¿†"""
        try:
            await memory_system.save_memory(
                user_id=user_id,
                companion_id=companion_id,
                memory_text=f"ç”¨æˆ·: {user_message}\nAI: {ai_response}",
                memory_type="conversation"
            )
            self.logger.info("ğŸ’¾ è®°å¿†å­˜å‚¨æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"è®°å¿†å­˜å‚¨å¤±è´¥: {e}")

    def _build_working_memory(
        self,
        conversation_history: Optional[List[Dict]]
    ) -> Optional[str]:
        """
        æ„å»ºL1å·¥ä½œè®°å¿†ï¼ˆå½“å‰å¯¹è¯ä¸Šä¸‹æ–‡æ‘˜è¦ï¼‰

        Args:
            conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]

        Returns:
            str: å·¥ä½œè®°å¿†æ‘˜è¦
        """
        if not conversation_history or len(conversation_history) == 0:
            return None

        # è·å–æœ€è¿‘çš„å¯¹è¯ï¼ˆæœ€å¤š5è½®ï¼Œ10æ¡æ¶ˆæ¯ï¼‰
        recent = conversation_history[-10:]

        # æ„å»ºç®€æ´çš„ä¸Šä¸‹æ–‡æ‘˜è¦
        context_lines = []
        for msg in recent:
            role = "ç”¨æˆ·" if msg["role"] == "user" else "ä½ "
            content = msg["content"][:100]  # é™åˆ¶é•¿åº¦
            context_lines.append(f"{role}: {content}")

        return "æœ€è¿‘å¯¹è¯:\n" + "\n".join(context_lines)

    async def _fallback_response(
        self,
        user_message: str,
        companion_name: str,
        debug_info: Dict,
        error_message: str
    ) -> CoordinatedResponse:
        """ç”Ÿæˆé™çº§å“åº”ï¼ˆå½“ä¸»æµç¨‹å¤±è´¥æ—¶ï¼‰"""
        self.logger.warning("âš ï¸ ä½¿ç”¨é™çº§å“åº”")

        # ä½¿ç”¨ç®€å•çš„LLMè°ƒç”¨ç”Ÿæˆå›å¤
        try:
            simple_prompt = f"ä½ æ˜¯{companion_name}ã€‚è¯·ç®€çŸ­ã€å‹å¥½åœ°å›å¤ç”¨æˆ·çš„æ¶ˆæ¯ã€‚"
            messages = [
                {"role": "system", "content": simple_prompt},
                {"role": "user", "content": user_message}
            ]
            ai_response = await llm_service.chat_completion(messages)
        except:
            ai_response = "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹ä¸åœ¨çŠ¶æ€...èƒ½å†è¯´ä¸€éå—ï¼Ÿ"

        # æ„å»ºæœ€å°åŒ–çš„å“åº”å¯¹è±¡
        from app.services.affinity_engine import EmotionAnalysis, ProcessResult
        from app.services.emotion_expression_generator import EmotionExpression

        minimal_emotion = EmotionAnalysis(
            primary_emotion="neutral",
            emotion_intensity=0.3,
            detected_emotions=[],
            user_intent="unknown",
            is_appropriate=True,
            violation_reason="",
            suggested_affinity_change=0,
            suggested_trust_change=0,
            suggested_tension_change=0,
            key_points=[],
            is_memorable=False
        )

        minimal_result = ProcessResult(
            emotion_analysis=minimal_emotion,
            affinity_change=0,
            trust_change=0,
            tension_change=0,
            new_affinity_score=0,
            new_trust_score=0,
            new_tension_score=0,
            new_level="stranger",
            new_level_name="é™Œç”Ÿ",
            level_changed=False,
            level_up=False,
            level_down=False,
            level_change_message="",
            response_guidance={},
            enhanced_system_prompt="",
            protection_warnings=[],
            protection_reason="",
            trend="",
            recovery_suggestion=""
        )

        minimal_expression = EmotionExpression(
            emotion_type="calm",
            emotion_category="neutral",
            intensity=0.3,
            intensity_level="low",
            verbal_style={},
            tone_guidance=[],
            suggested_phrases=[],
            punctuation_guide=[],
            body_language=[],
            facial_expression=[],
            voice_characteristics=[],
            response_structure={},
            opening_suggestions=[],
            closing_suggestions=[],
            affinity_level="stranger",
            intimacy_constraints={},
            user_intent="unknown",
            is_appropriate=True,
            adaptation_notes=[]
        )

        debug_info["errors"].append(f"Fallback activated: {error_message}")

        return CoordinatedResponse(
            ai_response=ai_response,
            emotion_analysis=minimal_emotion,
            process_result=minimal_result,
            emotion_expression=minimal_expression,
            system_prompt="[é™çº§æ¨¡å¼]",
            debug_info=debug_info
        )

    def serialize_response(self, response: CoordinatedResponse) -> Dict:
        """å°†å“åº”åºåˆ—åŒ–ä¸ºJSONå¯åºåˆ—åŒ–çš„å­—å…¸"""
        return {
            "ai_response": response.ai_response,
            "emotion_analysis": asdict(response.emotion_analysis),
            "process_result": asdict(response.process_result),
            "emotion_expression": asdict(response.emotion_expression),
            "system_prompt": response.system_prompt,
            "debug_info": response.debug_info
        }


# å…¨å±€å®ä¾‹
response_coordinator = ResponseCoordinator()
