"""
æµå¼ LLM æœåŠ¡
ä¸ºä¸æ”¯æŒåŸç”Ÿæµå¼çš„ LLM æä¾›æ¨¡æ‹Ÿæµå¼è¾“å‡º
"""
import asyncio
import re
from typing import AsyncIterator, List, Dict
from app.services.llm.factory import llm_service

class StreamingLLMService:
    """æµå¼ LLM æœåŠ¡åŒ…è£…å™¨"""
    
    def __init__(self):
        self.base_service = llm_service
    
    async def stream_chat_completion(
        self,
        messages: List[Dict[str, str]],
        system_prompt: str = "",
        temperature: float = 0.8,
        max_tokens: int = 300,
        **kwargs
    ) -> AsyncIterator[str]:
        """
        æµå¼èŠå¤©å®Œæˆ
        
        Args:
            messages: æ¶ˆæ¯å†å²
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Yields:
            str: æµå¼æ–‡æœ¬å—
        """
        try:
            # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºè¯ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯å¼€å¤´
            if system_prompt:
                full_messages = [{"role": "system", "content": system_prompt}] + messages
            else:
                full_messages = messages
            
            # è°ƒç”¨åŸºç¡€ LLM æœåŠ¡
            response = await self.base_service.chat_completion(
                messages=full_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            
            # å¦‚æœå“åº”æ˜¯é”™è¯¯æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›
            if not response or response.startswith("æŠ±æ­‰"):
                yield response or "æˆ‘ç°åœ¨æœ‰ç‚¹å›°æƒ‘ï¼Œèƒ½å†è¯´ä¸€éå—ï¼ŸğŸ˜…"
                return
            
            # å°†å“åº”åˆ†è§£ä¸ºè‡ªç„¶çš„æµå¼å—
            async for chunk in self._simulate_streaming(response):
                yield chunk
                
        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼š{str(e)} ğŸ˜…"
    
    async def _simulate_streaming(self, text: str) -> AsyncIterator[str]:
        """
        æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼Œæä¾›è‡ªç„¶çš„åˆ†å—ä½“éªŒ
        
        Args:
            text: å®Œæ•´çš„å›å¤æ–‡æœ¬
            
        Yields:
            str: æ–‡æœ¬å—
        """
        if not text:
            return
        
        # æ¸…ç†æ–‡æœ¬
        text = text.strip()
        
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰
        sentence_endings = r'[ã€‚ï¼ï¼Ÿ\.!?]+'
        sentences = re.split(f'({sentence_endings})', text)
        
        current_sentence = ""
        
        for i in range(0, len(sentences), 2):
            if i < len(sentences):
                sentence_content = sentences[i].strip()
                
                # è·å–æ ‡ç‚¹ç¬¦å·
                punctuation = ""
                if i + 1 < len(sentences):
                    punctuation = sentences[i + 1]
                
                if sentence_content:
                    current_sentence = sentence_content + punctuation
                    
                    # å¦‚æœå¥å­å¤ªé•¿ï¼ŒæŒ‰è¯åˆ†å‰²
                    if len(current_sentence) > 50:
                        words = current_sentence.split()
                        current_chunk = ""
                        
                        for word in words:
                            current_chunk += word + " "
                            
                            # æ¯10-15ä¸ªå­—ç¬¦å‘é€ä¸€æ¬¡
                            if len(current_chunk) >= 15:
                                yield current_chunk.strip()
                                await asyncio.sleep(0.08)  # æ§åˆ¶æµå¼é€Ÿåº¦
                                current_chunk = ""
                        
                        # å‘é€å‰©ä½™éƒ¨åˆ†
                        if current_chunk.strip():
                            yield current_chunk.strip()
                            await asyncio.sleep(0.2)  # å¥å­é—´åœé¡¿
                    else:
                        # çŸ­å¥å­ç›´æ¥å‘é€
                        yield current_sentence
                        await asyncio.sleep(0.3)  # å¥å­é—´åœé¡¿
    
    async def get_response_preview(
        self,
        messages: List[Dict[str, str]],
        system_prompt: str = "",
        max_length: int = 100
    ) -> str:
        """
        è·å–å›å¤é¢„è§ˆï¼ˆéæµå¼ï¼‰
        
        Args:
            messages: æ¶ˆæ¯å†å²
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_length: æœ€å¤§é¢„è§ˆé•¿åº¦
            
        Returns:
            é¢„è§ˆæ–‡æœ¬
        """
        try:
            if system_prompt:
                full_messages = [{"role": "system", "content": system_prompt}] + messages
            else:
                full_messages = messages
            
            response = await self.base_service.chat_completion(
                messages=full_messages,
                temperature=0.7,
                max_tokens=max_length
            )
            
            return response[:max_length] if response else "..."
            
        except Exception:
            return "é¢„è§ˆä¸å¯ç”¨"

# å…¨å±€æµå¼ LLM æœåŠ¡å®ä¾‹
streaming_llm_service = StreamingLLMService()
